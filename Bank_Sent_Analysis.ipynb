{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu-TDmuGPLhR"
   },
   "source": [
    "## Twitter Sentimental Analysis \n",
    "\n",
    "We would be analaysing how 1,000 random Nigerians feel about the commercial banks and digital banks they use.\n",
    "\n",
    "### Workflow \n",
    "\n",
    "* Create Twitter Developer account \n",
    "* Scrape the data from Twitter \n",
    "* Clean tweets \n",
    "* Analyze the Data \n",
    "* Data Visualizaton \n",
    "* Hypothesis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Using cached https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from tweepy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\toyin olape\\appdata\\roaming\\python\\python37\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from tweepy) (2.22.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.0)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\toyin olape\\appdata\\roaming\\python\\python37\\site-packages (from nltk>=3.1->textblob) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting WordCloud\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/f0/f7384c323c1fc7149573455f9633ef063c7b4d85c64d419b711bbca9ed29/wordcloud-1.8.1-cp37-cp37m-win_amd64.whl (154kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from WordCloud) (1.16.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from WordCloud) (3.1.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from WordCloud) (6.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from matplotlib->WordCloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from matplotlib->WordCloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from matplotlib->WordCloud) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from matplotlib->WordCloud) (2.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\toyin olape\\appdata\\roaming\\python\\python37\\site-packages (from cycler>=0.10->matplotlib->WordCloud) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\toyin olape\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->WordCloud) (41.0.1)\n",
      "Installing collected packages: WordCloud\n",
      "Successfully installed WordCloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GUOexl1mdBF3"
   },
   "outputs": [],
   "source": [
    "#import dependencies \n",
    "import tweepy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from textblob import TextBlob \n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\TOYIN\n",
      "[nltk_data]     OLAPE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\TOYIN\n",
      "[nltk_data]     OLAPE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "wordLemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "52nhflyTdsWR"
   },
   "outputs": [],
   "source": [
    "#Access Twitter API  \n",
    "\n",
    "key  =  \"MfWW96NNbgC2BP9gYvW0xNf3z\"\n",
    "secret = \"et5tSv0hq1xQbcZwgqgA7tQSK2xnvvPcFoU9ofihqf98cvyy7u\"\n",
    "\n",
    "access_token = \"366459074-DYjx7MdizB10YVQuxIVwBRniKvcASx6EKMZogHn6\"\n",
    "access_token_secret = \"j50GFMWpvpvGInQudjRMm6mAnM6Unm7gBjlWpov99Tq01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-jSkqjCkq5M9"
   },
   "outputs": [],
   "source": [
    "#Authenticate Twitter dev. access token\n",
    "auth = tweepy.OAuthHandler(key, secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSXbgpGexvRV"
   },
   "source": [
    "## Scope of Project\n",
    "General Sentimental Analysis of Commercial Banks and Digital Banks in Nigeria. An indepth analysis on the 5 biggest Commercial Banks in Nigeria. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw2CTzbeTGXd"
   },
   "source": [
    "## KUDA BANK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e6meMr7nsGM6"
   },
   "outputs": [],
   "source": [
    "#Parse twitter for Kuda bank tweets \n",
    "\n",
    "kuda_list = []\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search, q='kudabank-filter:retweets', language = \"en\", tweet_mode='extended',geocode ='10,7,1000mi').items(1500):\n",
    "    kuda_list.append(tweet.full_text)\n",
    "\n",
    "\n",
    "df_kuda = pd.DataFrame(kuda_list, columns=['Tweets'])\n",
    "df_kuda.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kUnIqD8boaBA",
    "outputId": "63d59949-d32f-42f1-904f-bb31b2f9cc87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1412, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kuda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@OwoborodeBiodun @GbengaGOLD @Babsogundeyi @Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Alexcassy @FS_AYODELE @GbengaGOLD @Babsogunde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@FS_AYODELE @OwoborodeBiodun @GbengaGOLD @Babs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@kudabank If you live in owerri like this twét</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kudabank pls help with my account password , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  @OwoborodeBiodun @GbengaGOLD @Babsogundeyi @Ni...\n",
       "1  @Alexcassy @FS_AYODELE @GbengaGOLD @Babsogunde...\n",
       "2  @FS_AYODELE @OwoborodeBiodun @GbengaGOLD @Babs...\n",
       "3     @kudabank If you live in owerri like this twét\n",
       "4  @kudabank pls help with my account password , ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kuda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have so much useless data to remove it \n",
    "\n",
    "we need to clean the data\n",
    "\n",
    "for cleaning we are going to use `re` (regex) \n",
    "\n",
    "process : \n",
    "    1. Lower Casing: Each text is converted to lowercase.\n",
    "    2. Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n",
    "    3. Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n",
    "    4. Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n",
    "    5. Removing Non-Alphabets: [^a-zA-z]\n",
    "    6. Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n",
    "    7. Removing Short Words: Words with length less than 2 are removed.\n",
    "    8. Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
    "    9. Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: “Great” to “Good”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJIS = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "URLPATTERN        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "USERPATTERN       = '@[^\\s]+'\n",
    "SEQPATTERN   = r\"(.)\\1\\1+\"\n",
    "SEQREPLACE = r\"\\1\\1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URL URL URL URL URL URL URL URL mi ni kuda account abi wa fun mi ni owo ti fi si kuda account'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_kuda = []\n",
    "for texts in df_kuda[\"Tweets\"]:\n",
    "    \n",
    "    text = texts.lower()\n",
    "    ### Replacing URL\n",
    "    text = re.sub(URLPATTERN,' URL',text)\n",
    "    ### Replacing EMOJI\n",
    "    for emoji in EMOJIS.keys():\n",
    "        text = text.replace(emoji, \"EMOJI\" + EMOJIS[emoji])  \n",
    "    ### Replacing USER pattern\n",
    "    text = re.sub(USERPATTERN,' URL',text)\n",
    "    ### Removing non-alphabets\n",
    "    text = re.sub('[^a-zA-z]',\" \",text)\n",
    "    ### Removing consecutive letters\n",
    "    text = re.sub(SEQPATTERN,SEQREPLACE,text)\n",
    "    text = text.split()\n",
    "    text = [wordLemm.lemmatize(word) for word in text if not word in stopwords.words('english') and len(word) > 1]\n",
    "    text = ' '.join(text)\n",
    "    corpus_kuda.append(text)\n",
    "\n",
    "corpus_kuda[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1412, 3000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CV = CountVectorizer(max_features=3000,ngram_range=(1,2))\n",
    "df_kuda = CV.fit_transform(corpus_kuda).toarray()\n",
    "\n",
    "df_kuda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "SnUBxhyTolY5"
   },
   "outputs": [],
   "source": [
    "#Since i am going to parse for several banks. I will go aheadd and change the above code to a function\n",
    "def parse_tweet(keyword):\n",
    "  listname = []\n",
    "\n",
    "  for tweet in tweepy.Cursor(api.search, q= keyword + '-filter:retweets', language = \"en\",tweet_mode='extended',geocode ='10,7,1000mi' ).items(1500):\n",
    "    listname.append(tweet.full_text)\n",
    "  \n",
    "  return listname\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, i will make the cleaning function in order to cleaning the other data sets \n",
    "import time\n",
    "def clean_tweets(listname):\n",
    "    \n",
    "    t = time.time()\n",
    "    corpus = []\n",
    "    for texts in listname:\n",
    "        ## lower casing\n",
    "        text = texts.lower()\n",
    "        ### Replacing URL\n",
    "        text = re.sub(URLPATTERN,' URL',text)\n",
    "        ### Replacing EMOJI\n",
    "        for emoji in EMOJIS.keys():\n",
    "            text = text.replace(emoji, \"EMOJI\" + EMOJIS[emoji])  \n",
    "        ### Replacing USER pattern\n",
    "        text = re.sub(USERPATTERN,' URL',text)\n",
    "        ### Removing non-alphabets\n",
    "        text = re.sub('[^a-zA-z]',\" \",text)\n",
    "        ### Removing consecutive letters\n",
    "        text = re.sub(SEQPATTERN,SEQREPLACE,text)\n",
    "        text = text.split()\n",
    "        text = [wordLemm.lemmatize(word) for word in text if not word in stopwords.words('english') and len(word) > 1]\n",
    "        text = ' '.join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus\n",
    "    print(f'Time Taken: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(corpus):\n",
    "\n",
    "    CV = CountVectorizer(max_features=3000,ngram_range=(1,2))\n",
    "    df_1 = CV.fit_transform(corpus).toarray()\n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt4pP32TL36f"
   },
   "source": [
    "## GTB Bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "h1wY9n-m8PNj",
    "outputId": "35cea13e-3fb1-4394-cac3-ac390ec5eb1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If Beyonce was born in Nigeria in 1981, there'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@kayzywizzzy 0210132521 gtb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most capitalized companies on NSE as at 19th M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@cuppymusic @BetKingNG In case the money show....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@crayonthis Dm not open\\n\\nGtb\\n0414086570\\n🙌😍...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  If Beyonce was born in Nigeria in 1981, there'...\n",
       "1                        @kayzywizzzy 0210132521 gtb\n",
       "2  Most capitalized companies on NSE as at 19th M...\n",
       "3  @cuppymusic @BetKingNG In case the money show....\n",
       "4  @crayonthis Dm not open\\n\\nGtb\\n0414086570\\n🙌😍..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtb_list = parse_tweet(\"gtb\")\n",
    "\n",
    "df_gtb = pd.DataFrame(gtb_list1, columns=['Tweets'])\n",
    "df_gtb.drop_duplicates(inplace = True)\n",
    "df_gtb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gtb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "38yN6gGxMIRG"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-7378cf241227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcorpus_gtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df_gtb[\"Tweets\"]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorpus_gtb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "corpus_gtb = clean_tweets('df_gtb[\"Tweets\"]')\n",
    "corpus_gtb[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@cuppymusic @BetKingNG Aunty cuppy no fear we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Mbahdeyforyou I for send the remaining #100 t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Abbeytips101 I don drop Aza 3102072331 firstbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You see how cute the iPhone 12 is, one person ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Try out the new FirstMobile App from FirstBank...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  @cuppymusic @BetKingNG Aunty cuppy no fear we ...\n",
       "1  @Mbahdeyforyou I for send the remaining #100 t...\n",
       "2  @Abbeytips101 I don drop Aza 3102072331 firstbank\n",
       "3  You see how cute the iPhone 12 is, one person ...\n",
       "4  Try out the new FirstMobile App from FirstBank..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_list = parse_tweet(\"firstbank\")\n",
    "\n",
    "df_fb = pd.DataFrame(fb_list, columns=['Tweets'])\n",
    "df_fb.drop_duplicates(inplace = True)\n",
    "df_fb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fb = clean_tweets('df_fb[\"Tweets\"]')\n",
    "corpus_fb[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenith Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 10 Banks on the NSE by Market Cap as at 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@JohnsonDicksonO @ZenithDirect_ Kindly be info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@zenithbank91 Kindly be informed that we prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@_AsiwajuLerry Omo e belike u go send me money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Duki11223879 @cenbank Please remember that Ze...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  Top 10 Banks on the NSE by Market Cap as at 19...\n",
       "1  @JohnsonDicksonO @ZenithDirect_ Kindly be info...\n",
       "2  @zenithbank91 Kindly be informed that we prese...\n",
       "3  @_AsiwajuLerry Omo e belike u go send me money...\n",
       "4  @Duki11223879 @cenbank Please remember that Ze..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zb_list = parse_tweet(\"zenithbank\")\n",
    "\n",
    "df_zb = pd.DataFrame(zb_list, columns=['Tweets'])\n",
    "df_zb.drop_duplicates(inplace = True)\n",
    "df_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_zb = clean_tweets('df_zb[\"Tweets\"]')\n",
    "corpus_zb[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## United Bank of Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Toshibasexy @fkeyamo I have not received any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Irunnia_ Boss me dy always dy active\\n🙏🏾💯\\n21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@blavk_pharouqq Kace bara'uba kawai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Irunnia_ Bless me sir this night ,2163017201 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Irunnia_ 2145013046\\nUBA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  @Toshibasexy @fkeyamo I have not received any ...\n",
       "1  @Irunnia_ Boss me dy always dy active\\n🙏🏾💯\\n21...\n",
       "2                @blavk_pharouqq Kace bara'uba kawai\n",
       "3  @Irunnia_ Bless me sir this night ,2163017201 ...\n",
       "4                          @Irunnia_ 2145013046\\nUBA"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uba_list1 = parse_tweet(\"UBA\")\n",
    "\n",
    "df_uba2 = pd.DataFrame(uba_list1, columns=['Tweets'])\n",
    "df_uba2.drop_duplicates(inplace = True)\n",
    "df_uba2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_uba = clean_tweets('df_uba2[\"Tweets\"]')\n",
    "corpus_uba[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@accessbank_help kindly refer to my DMs that I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Jaru0 Hello Jaru0, we have replied your Dm. K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pajoil25 Hi pajoil25, please check your DM. \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@accessbank_help please check your DM thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@EmekaBennard Hello @EmekaBennard, thanks for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  @accessbank_help kindly refer to my DMs that I...\n",
       "1  @Jaru0 Hello Jaru0, we have replied your Dm. K...\n",
       "2  @pajoil25 Hi pajoil25, please check your DM. \\...\n",
       "3       @accessbank_help please check your DM thanks\n",
       "4  @EmekaBennard Hello @EmekaBennard, thanks for ..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_list = parse_tweet(\"AccessBank\")\n",
    "\n",
    "df_ab = pd.DataFrame(ab_list, columns=['Tweets'])\n",
    "df_ab.drop_duplicates(inplace = True)\n",
    "df_ab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ab = clean_tweets('df_ab[\"Tweets\"]')\n",
    "corpus_ab[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5UK-YcCQF8f"
   },
   "source": [
    "## Assumptions & Challenges \n",
    "\n",
    "The data collected for first bank would also be quite messy, because the word \"first\" can be found with other banks and therefore it means there would be  lot of noise in the data set collected for First bank. \n",
    "\n",
    "Collecting the data for Kuda Bank at this time is heavily influenece by the fact that they just recieved a Series A funding of $25 million, Majority of the tweets containing this keyword would be congratolatory and therefore Positive \n",
    "\n",
    "Nigeria is currently the poverty capital of the world and as such most of the tweets are of people posting their account number in order to recieve funds from kind strangers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdA0BUK_s6wA"
   },
   "outputs": [],
   "source": [
    "#clean the text\n",
    "def cleantxt(text):\n",
    "  text = re.sub(r'@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Work \n",
    "\n",
    "Traingulate the tweets according to there locations to better understand the areass with negative tweets in order to improve bank services and improve the overall positive sentiment. \n",
    "\n",
    "Filter out give away tweets to reduce the noise "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bank_Sent_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}